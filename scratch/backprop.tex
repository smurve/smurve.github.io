\documentclass[]{report}

\usepackage{color}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{courier}
\lstset{
	basicstyle=\footnotesize\ttfamily, % Standardschrift
	%numbers=left,               % Ort der Zeilennummern
	numberstyle=\tiny,          % Stil der Zeilennummern
	%stepnumber=2,               % Abstand zwischen den Zeilennummern
	numbersep=5pt,              % Abstand der Nummern zum Text
	tabsize=2,                  % Groesse von Tabs
	extendedchars=true,         %
	breaklines=true,            % Zeilen werden Umgebrochen
	keywordstyle=\color{red},
	frame=b,         
	%        keywordstyle=[1]\textbf,    % Stil der Keywords
	%        keywordstyle=[2]\textbf,    %
	%        keywordstyle=[3]\textbf,    %
	%        keywordstyle=[4]\textbf,   \sqrt{\sqrt{}} %
	stringstyle=\color{white}\ttfamily, % Farbe der String
	showspaces=false,           % Leerzeichen anzeigen ?
	showtabs=false,             % Tabs anzeigen ?
	xleftmargin=17pt,
	framexleftmargin=17pt,
	framexrightmargin=5pt,
	framexbottommargin=4pt,
	%backgroundcolor=\color{lightgray},
	showstringspaces=false      % Leerzeichen in Strings anzeigen ?        
}
\lstloadlanguages{% Check Dokumentation for further languages ...
	Java
}

\title{Yet Another Introduction to Neural Networks}
\author{Wolfgang Giersche}

\begin{document}
	
\begin{titlepage}
\Large{Yet Another Introduction to Neural Networks}

\bigskip

Wolfgang Giersche

\bigskip

 2017
\end{titlepage}

\part{Preface}

\emph{When software engineering meets machine learning...}

\bigskip

A lot of introductory material has been written and is still being written about neural networks these days, and some of the available material has reached an unbelievably high quality in both depth and accessibility, which is a rare combination. First and foremost, I must mention [Michael Nielson's online book](http://neuralnetworksanddeeplearning.com/) \cite{nielson2015}
and [Chris Olah's blog](http://colah.github.io/) \cite{olah2015}, full of really beautiful reasoning, and Ian Goodfellow's popular text book ["Deep Learning"] (http://www.deeplearningbook.org/) \cite{goodfellow}. 
So, as an analogy to the well-known saying "I'm standing on the shoulders of giants", I can only conclude that I am "wandering between the toes of giants". Hence, I will often-times refer the reader to one of these giants. 
 
So, why yet another introductory text?

As I was curiously wandering through the internet on my quest to understand machine learning, I found a lot of beautiful math, beautiful concepts and thorough, even more beautiful reasoning (I know I mentioned that already), but in the end there was always a grain of salt. The provided code examples somehow never truly appealed to me. That may be in parts because of personal gusto - admittedly. But it's definitely not only because of the used language. Python is pretty much ok, I'm getting used to it. 

It's because - as a software engineer - I learned to love composable solutions so much. So what this blog is going to be about is - in a nutshell - a software engineering approach to neural networks: Going from requirements (math) to maintainable code and reusable componenents. As an extra, you'll see how vector algebra can be seen as the perfect link to design and develop high performance algorithms.

\bigskip

In September 2017, I visited the EARL conference in London. EARL stands for "Enterprise Applications of the R Language". Joe Cheng of RStudio (actually a pretty brilliant lad, as it seemed), made some jokes about the verbose naming that you typically find in Java and similar communities. He said something like \emph{I called this thing 'Promise' - in Java they would probably have called it AbstractPromiseAdapter or so...}. Everybody had a good laugh. Well - everybody but me! That verbose naming is there for very good reasons, I thought. It became obvious to me that data analytists and machine learning enthusiasts have some distance to bridge, before they can work together seamlessly. As a side note, though: Sometimes we software engineers, indeed, tend to exaggerate it, don't we?

However, the good news that I brought back with me from the conference is that a) R Shiny is really a pretty cool tool and b) the R community does in fact start to realize the need for more of what we software engineers call enterprise readiness. Testing and version control have been popular topics during the conference. I could see that the other side - the data analytics community - is already aware of the changes, and moving in for the merge. And: yes, we software developers are warming up to the practice of explorative coding, too. Scala enjoyed a scripting environment called REPL since its inception, and JAVA 9 now also comes with a REPL, at last. I wonder, though, whether we software engineers are sufficiently aware of the fact that the cool kid - machine learning - comes with a pretty demanding friend: math!

\bigskip

Data analytics and software engineering are two different academical fields, that - obviously - share common habits. For example, they both involve some kind of programming to solve their particular problems. But their respective approaches to programming are sometimes radically different. So let's compare them very shortly:

The problems within data analytics are mostly expressed in terms of mathematics. Data analysts often call themselves  scientists (as opposed to \emph{engineers}). Their programming experience is mainly determined by the need to easily express those mathematical concepts and explore the plethora of algorithms available to them to find the best solution to a given problem. Data analytics has a strong exploratory flavor and the typical tools and languages, such as mathlab, R, or Python, support expecially those needs. 

Software engineering, however, usually is more concerned with modeling. Modelling some existing part of the real world - mostly the business world. Many times they're taking into consideration the human interaction with that part of the world. Engineers use patterns and principles, taking into consideration the entire life cycle of a software product. They often call themselves \emph{craftsmen} and they love to talk about maintainability, non-functional requirements and javascript frameworks.

\bigskip

These two worlds coexisted in peace and mutual ignorance for decades (Exaggerating, of course ;-). Now, with machine learning on the very maximum of Gartners hype cycle \cite{gartner2017}, however, inevitably more and more software products will have to leverage some kind of machine learning to keep up with growing expectations. Data analysts' artifacts will have to share the application code's entire lifecycle to be incorporated into maintainable software products. Data analysts and software engineers will have to talk more, understand each other better, and most importantly, start to respect each others' programming habits. Eventually, I believe, the respective fields will even merge together. I believe that I'm spotting signs that we're already building roads for that big merger in the success of the Scala language. In his well-received talk in New York in 2016, Dean (Snow) Wampler of Lightbend eloquently called Scala "the unpredicted lingua franca of data science" \cite{wampler2016}. Certainly, Dean is somewhat biased, as his company has some stakes in the language. But he made some very good points. I'll be using the potential of Scala's ecosystem in the accompanying source code for this blog to prove that point. You'll see: It can be used exploratorily and expressively when dealing with mathematical concepts, and still easily produces well-designed production-ready artifacts when closing in on production deployments.

\bigskip

Just like in almost every other introductory text, in this post I will solve the problem of recognizing hand-written digits. Yes - I know it's rather boring, but since the problem itself it not at the heart of the matter but rather the way \emph{how} we solve it considering some of the lessons-learned within the reign of software engineering, I hope you don't mind too much.

So, it's about the MNIST - Machine Learning's "Hello World" again, in which respect it's just \emph{Yet Another Introduction to Neural Networks}, hence the title of this post: YAINN. In some respect though, I'll follow a different approach: I firmly assume you have some good understanding of - and interest in - programming (preferrably, but not necessarily in Scala), and you have some foundational knowledge of calculus and linear algebra and mathematical notation in general. And you definitely should have seen, although not necessarily need to fully understand Michael Nielson's excellent introductory blog \cite{nielson2015} and Chris Olah's breath-taking art of sense-making and visualization \cite{olah2015}. Machine learning comes with a wealth of best practices that I won't repeat here, simply because Michael and Chris - amongst many others - have already done a splendid job. 

Contrary to Michael and most other authors, though, in this post I will intentionally avoid the typical biological metaphors wherever possible. So - no neurons firing in this blog post. Of course, I can't avoid all of the terminology, so I will occasionally fall back to the well-known lingo where otherwise I'd jeopardize your understanding of the context.

\bigskip

Before I dive into the math and scare you off for the rest of time: Like programming languages, math is a way to precisely describe what has to be programmed and calculated. If you're a software developer: In case you happen to find yourself in the middle of a truly innovative machine learning project, you will not so unlikely be concerned with some math at some point in time. The data scientist of your team may turn to you and ask you to implement some algorithm within the product you're developing. If you're lucky you may succeed without understandig and just with manually transpiling her R or python script. If not, you may not make it in the team for too long. Shame - such a cool project!

The math I present here may look difficult at times. Believe me, it's not. Not at all. We're just not used to it (anymore). Math is simply unmisunderstandably precise - and extremely concise. Things wouldn't work otherwise. And look: the things I present here are not even close to \emph{rocket science}. It's applied undergraduate level math at best. The key to understanding this post - and in general to getting somewhere in the more scientific part of software engineering is often not intelligence - but rather dedication, perserverance and a hell lot of concentration on the subject. Ready for it?

\bigskip 

disclaimer: This post is not about...

Well, then let's get started.
 
 \part{Early Success With a Naive Approach}
 
 \chapter{A Function to Optimize}
 

Neural networks can solve - amongst others - a particular category of problems, now mostly referred to as \emph{classification} problems. A classification can be seen as just one special interpretation of a mathematical function. An arbitrary function \(f(x): R^m \rightarrow R^n\) that maps an input vector \(\mathbf{x} \in R^m\) to an output vector \(\mathbf{y} \in R^n\):

\begin{equation}
	\mathbf{y}=f(\mathbf{x})
\end{equation}

may be interpreted as a classification function, if that resulting vector is somewhat close to a so-called \emph{one-hot} vector where only a single component has the value 1 and all others are zero, . For example, to say that some input would belong to class number 4 of 10 possible classes, the perfect resulting value for \(\mathbf{y}\) would be

\begin{equation}
\mathbf{y} = (0,0,0,1,0,0,0,0,0,0)
\end{equation}

In reality you'll rather find values similar to

\begin{equation}
\mathbf{y} = (0.08, 0.10, 0.03, 0.86, 0.03, 0.04, 0.00, 0.04, 0.07, 0.09)
\end{equation}

We will interpret the above and similar results as \emph{most probably class 4}. Now, with this interpretation, our problem can be rephrased to: "Find the particular function \(f(\mathbf{x})\) that always returns the \emph{one-hot} vector that corresponds to the digit represented by the given image."

The image itself, of course, will have to be represented by a vector too. In case of the MNIST data set, it's a vector of \(28 \times 28 = 784\) byte. In this post, I will not exploit the geometrical structure of the image, so we just align all the rows of 28 bytes each to a single vector of 784 bytes. Michael Nielson explains pros and cons of that approach in his text better than I could ever do.

So the problem now has a precise mathematical form: Find the function \(f(x): R^m \rightarrow R^n\) that maps any given vector \(\mathbf{x} \in R^{784}\) representing a particular image to an output vector \(\mathbf{y} \in R^{10}\) that can be interpreted as a classification to the digit that is represented by that image.

\bigskip

Since exploring an unlimited number of possible functions is a daunting task, we follow a well-known problem solving strategy and start by reducing the search space. The very early candidates for neural networks were constructed as affine functions with some non-linear component-wise so-called activation function (Rosenblatt, Mcculoch, Pitts). That's still one of the most popular ingredients of neural networks, so let's stick to those and use

\begin{equation}
\mathbf{f}(\mathbf{x})=\Phi(\mathbf{W} \cdot \mathbf{x} + \mathbf{b})
\end{equation}

where \(\mathbf{W}\) is an \(n \times m\) \emph{weight matrix}, \(\mathbf{b} \in R^m\)  is a vector called \emph{bias} and finally, \( \Phi( \mathbf{x})  \) is a function that is defined as the component-wise application of a scalar, non-linear \emph{activation function} \(\phi(x)\).

\begin{equation}
\Phi(z_1, z_2, \cdots, z_{n-1}, z_n) =
\left(
\begin{array}{l}
\phi(z_1) \\
\phi(z_2) \\
\cdots \\
\phi(z_{n-1}) \\
\phi(z_n) \\
\end{array}
\right)
\end{equation}


We will eventually use the \emph{sigmoid} function, which is one of the steady and (almost everywhere) differentiable functions described in the relevant literature. But for this text, it's actually irrelevant.

Please note that I have clandestinely introduced a typical convention here: Variables in bold-face represent vectors (lower case) and matrices (upper-case). Regular type variables represent scalars, such as the components shown above. One more bit of lingo here: In the proceding, we'll call that function \( f( \mathbf{x})  \) our \emph{neural network model}, or simply \emph{the model}. The parameters of \( f( \mathbf{x})\), once tuned, somehow embody an \emph{understanding} of the meaning of hand-written digits, that's why you can say they represent a model of those digits.

\bigskip

Let's wrap up: It looks like if we find all the parameters of a \(784 \times 10 \) matrix and a 10-dimensional bias, we could try and see whether we solve the problem with those. Good! Then, how can we determine those 7840 + 10 parameters? It turns out that this is not so difficult. If we had a so-called \emph{loss function} that for all images provides us with a scalar measure of how far we are from the perfect solution, we could use that function to go with the following procedure or algorithm:

\begin{itemize}
\item for all images, calculate the current value of the given loss function
\item tweek one single parameter at a time
\item If the loss function decreases, continue with the next parameter
\item If the loss function increases, tweek back and a little bit more in the other direction and continue
\item continue through all weights and biases multiple times, until the loss function doesn't improve any more
\end{itemize}

Saying the same in more technical terms (here, exemplary only for the weights):

\begin{equation} \label{algo}
\begin{array}{l l l}
\forall \mathbf{x} \in R^m, \bar{\mathbf{y}} \in R^n: \\
\textrm{Let } \mathbf{W}_i \textrm{ be the i-th iteration of all weights} \\
\textrm{Let } c_i = \mathcal{D}(\mathbf{W_i}, \mathbf{b},  \mathbf{x}, \bar{\mathbf{y}}) \textrm{ be the current cost}\\
\textrm{Let } c_i^{(+)} = \mathcal{D}(\mathbf{W_i}+\Delta\mathbf{W}, \mathbf{b},  \mathbf{x}, \bar{\mathbf{y}}) \\
\textrm{Let } c_i^{(-)} = \mathcal{D}(\mathbf{W_i}-\Delta\mathbf{W}, \mathbf{b},  \mathbf{x}, \bar{\mathbf{y}}) \\
\textrm{if} c_i^{(+)} < c_i^{(-)} \textrm{then} \mathbf{W_{i+1}}=\mathbf{W_i}+\Delta\mathbf{W} \\
\textrm{if} c_i^{(+)} > c_i^{(-)} \textrm{then} \mathbf{W_{i+1}}=\mathbf{W_i}-\Delta\mathbf{W}
\end{array}
\end{equation}

To establish such a loss function, we need to know the correct classification in advance. Because then the loss function could simply compare the correct classification to the result of applying the model to all the images.

Not surprisingly, it turns out that the Euclidean distance (or better, it's square) is a pretty good candidate. The particular distance we're talking about is the sum of all the distances between the respective known true classification \(\mathbf{\bar{y}} \) of an image and what our model function produces when applied to the respective image. In math terms the loss function \(\mathcal{D}\) as a function of the weights and the bias looks like

\begin{equation} \label{eq:60}
\mathcal{D}( \mathbf{W}, \mathbf{b}) = \frac{1}{2} \cdot \sum_{i=1}^{N}
(\Phi(\mathbf{W} \cdot \mathbf{x}_i + \mathbf{b}) - \mathbf{\bar{y}}_i)^2
\end{equation}

Here, the factor of \(\frac{1}{2} \) is mere convenience as you'll see soon, and N is the total number of images with known classifications \(\mathbf{\bar{y}} \), the latter we will mostly refer to as \emph{labels} from now on. Note, that the squaring denotes the inner product of a vector with itself, resulting in the sum of the squares of the components:

\begin{equation}
	\mathbf{x}^2 = \sum_{i=1}^{10}x_i^2
\end{equation}

with \(10\) being the dimesion of the classification vectors. By the way, finding a model this way, based on the knowledge of a large number of true result, is called \emph{supervised} learning. So this is what our model is going to undergo now: Supervised learning by optimizing the given function with respect to our chosen euclidean cost function. Of course, the euclidean ist just one possible functions Any function with the aforementioned characteristic - namely being scalar and indicative to the quality of the output - can do, and indeed other functions have been found more efficient with certain models. We stick to the euclidean simply because it is the simplest.

\bigskip

\chapter{A Gradient to Descend }

Time for some more math! The algorithm for tweeking and tuning the parameters [\ref{algo}] that we sketched in the previous chapter turns out to be very inefficient. The problem with tweeking single parameters is that you can only do it one single parameter at a time thus requiring a lot of iterations. That may still work somehow for our simple model - though probably painfully slow - but later on our journey that'll just not get us anywhere anymore. And this is where calculus comes to the rescue. Calculus is about calculating what would happen if we tuned some parameter this or that way. Since we're not actually tweeking parameters during the computations of calculus we can compute the expected change of the distance function for all parameters in a single go. The result of this calculation is itself a matrix or vector - one component for each parameter - telling us how much the cost will change when we change that parameter a tiny little, more precisely an infinitesimally small bit. This resulting matrix or vector is called the \emph{gradient} of the cost function. The gradient can geometrically be interpreted as a vector that points into the direction of steepest ascent. Thus adding a fraction of the gradient to our parameters would increase the cost function. We'll do the opposite, obviously - and subtract a fraction of the gradient to find the next, slightly better parameter set, i.e. yielding a lower value for the distance or cost. Doing that for a number of times until the distance function doesn't improve anymore will yield the desired result: A function that computes (more or less) exactly the results that we expect. A function that has learned to recognize images! However, exactly how good the function will be depends on a number of aspects that we will explore later in this text. For now, let's find the best we can get with a single matrix of weights and a vector of biases.

\bigskip

Let's wrap up: We're setting out to find the gradient of the cost function because that will show us how - given an arbitrary set of parameters - we can find a slightly better set, a set that produces a classification closer to the ideal, true classification. Ok? We're on a purposeful mission.

In the following, we denote the gradient with respect to the weights and the bias by \( \nabla_{\mathbf{W}} \) or \( \nabla_{\mathbf{b}}\) respectively. The triangle operator is called \emph{nabla}. With \(w_{ij}\) being the parameter of \(\mathbf{W} \) at row \(i\) and column \(j\) and \(b_k\) being the k-th component of the bias vector, the gradient with respect to each parameter is defined by the following \emph{partial derivate} expressions

\begin{equation}\label{eq:80}
	\nabla_{\mathbf{W}} \mathcal{D}( \mathbf{W}, \mathbf{b})= \frac{\partial}{\partial w_{ij}}  \mathcal{D}( \mathbf{W}, \mathbf{b})
\end{equation}

\begin{equation}\label{eq:81}
	\nabla_{\mathbf{b}} \mathcal{D}( \mathbf{W}, \mathbf{b}) = \frac{\partial}{\partial b_k} \mathcal{D}( \mathbf{W}, \mathbf{b})
\end{equation}

To actually perform the calculation, we need to re-write the distance function in equation \ref{eq:60} in terms of its components:

\begin{equation}\label{eq:100}
\mathcal{D}(w_{11},\cdots, b_1, \cdots) = \frac{1}{2} \sum_{k=1}^N \sum_{m=1}^{10} ( \phi(\sum_{n=1}^{784} w_{mn} x^{(k)}_n + b_m) - \bar{y}_m^{(k)})^2
\end{equation}

where \(x^{(k)}_n \) denotes the n-th component of the vector representing the k-th image and \(\bar{y}_m^{(k)}\) - in the same fashion - stands for the m-th component of the k-th label vector.
This looks much more difficult than it actually is. When we finally get at the implementation in Scala, we'll make use of the more convenient vector algebra representation. We'll use a particular library called ND4J to do the majority of all index juggling for us. So, you'll see much fewer indices then. However, we do need this component-wise representation now to correctly calculate the derivatives defined in \ref{eq:80} and \ref{eq:81}. Once we have the result, we'll rephrase it back to the simpler vector notation and use only the latter in code. This is not only for our convenience or to end up with more readable and maintainable code. It also allows for much more efficient algorithms to do the actual matrix multiplications behind the scene. Exactly  that's what ND4J is going to do for us. So please keep in mind that what we're going to go through now will eventually ease our lives significantly.

\bigskip

One last thing: Before we start the big number crunching here, let me introduce a little technique that we physicists used to make our lives easier. It involves the delta symbol \(\delta_{ij}\) that we define as being 1 for \(i=j \) and 0 otherwise. It allows for the following nice tricks:

\begin{equation}\label{eq:110}
\frac{\partial b_i}{\partial b_j} = \delta_{ij}
\end{equation}

and similarly, obviously

\begin{equation}\label{eq:120}
\frac{\partial w_{ij}}{\partial w_{km}} = \delta_{ik} \cdot \delta_{jm}
\end{equation}

and the best thing is: it collapses sums, because all the terms containing a factor \(\delta_{ij}\) simply disapper for all \(  i \neq j\). Watch how this allows us to calculate derivatives of matrix products

\begin{equation}\label{eq:130}
\frac{\partial}{\partial x_k} \sum_j w_{ij} \cdot x_j = \sum_j w_{ij} \cdot \frac{\partial x_j}{\partial x_k}
=\sum_j w_{ij} \cdot \delta_{jk} = w_{ik}
\end{equation}

Cool, isn't it?

\bigskip


Now back to the original problem. We'll apply the chain rule twice, dropping constant terms on the fly, and eventually use the \(\delta \)-function.

\begin{equation}\label{eq:140}
	\frac{\partial}{\partial w_{ij}}   \frac{1}{2} \sum_{k=1}^N \sum_{m=1}^{10} ( \phi(\sum_{n=1}^{784} w_{mn} x^{(k)}_n + b_m) - \bar{y}_m^{(k)})^2 =
\end{equation}



\begin{equation}\label{eq:150}
 \sum_{k=1}^N \sum_{m=1}^{10}
(\phi(\sum_{n=1}^{784} w_{mn} x^{(k)}_n + b_m) - \bar{y}_m^{(k)})
\cdot
\frac{\partial}{\partial w_{ij}}
(\phi(\sum_{n=1}^{784} w_{mn} x^{(k)}_n + b_m) - \bar{y}_m^{(k)})
\end{equation}

To achieve the above result, we applied the chain rule \((f^2)' = 2 \cdot f \cdot f' \). And now using \( y^{(k)}_m\) instead of our original model function, namely
\begin{equation}
y^{(k)}_m = \phi(\sum_{n=1}^{784} w_{mn} x^{(k)}_n + b_m)
\end{equation}

and using the chain rule for arbitrary functions \( f(g)' = f'(g) \cdot g' \)

\begin{equation}\label{eq:170}
= \sum_{k=1}^N \sum_{m=1}^{10}
(y_m^{(k)} - \bar{y}_m^{(k)})
\cdot
\phi^{\prime}(y_m^{(k)}) \cdot
\frac{\partial}{\partial w_{ij}}\sum_{n=1}^{784} w_{mn} x^{(k)}_n
\end{equation}

\begin{equation}
=\sum_k \sum_m \sum_n
(y_m^{(k)} - \bar{y}_m^{(k)})
\cdot
\phi^{\prime}(y_m^{(k)}) \cdot
\delta_{im} \cdot \delta_{jn} \cdot x^{(k)}_n
\end{equation}

\begin{equation}
= \sum_k
(y_i^{(k)} - \bar{y}_i^{(k)})
\cdot
\phi^{\prime}(y_i^{(k)}) \cdot
x^{(k)}_j
\end{equation}

Since all our vectors are meant to be column vectors, the inner product is expressed by the 'dot' product with a transposed vector to the left like in \(c = \mathbf{a}^T \cdot \mathbf{b} \) - the result being a scalar, and the outer product is expressed by the 'dot' product with a transposed to the right like in \(\mathbf{M} = \mathbf{a} \cdot \mathbf{b}^T \), the result being a matrix this time.
So now, with also the additional help of the Hadamard product symbol \(\odot \)  denoting component-wise multiplication, and the \(\nabla\) symbol for the gradient, we can confidently return to vector notation

\begin{equation} \label{eq:180}
\nabla_{\mathbf{W}} \mathcal{D}(\mathbf{W}, \mathbf{b})=
\sum_{k=1}^N \left[ ( \mathbf{y}_k - \bar{\mathbf{y}}_k    )  \odot \mathbf{\Phi^{\prime}}(\mathbf{y}_k) \right]
\cdot \mathbf{x}_k^T
\end{equation}

Look at this beautiful result: We can see that the gradient constitutes of three independent components, that simply need to be multiplied the correct way. Of course, this is exactly what the chain rule says, but without going through the effort of component-wise calculation, we probably wouldn't have confidently found that correct way of multiplying these components.

Here, \( \mathbf{y}_k - \bar{\mathbf{y}}_k\) stems from the distance function, \( \mathbf{\Phi^{\prime}}(\mathbf{y}_k)\) from the activation function and \(\mathbf{x}_k\) from the matrix multiplication. Looking ahead I can tell you that the vector notation will also come in handy when we attempt to understand backpropagation, a very important concept of deep learning, later in this blog.

One last thing to do is to calculate the gradient with respect to the bias, for which we simply start by looking at \ref{eq:170} and replace the partial differentiation with the one for the bias:

\begin{equation}\label{eq:210}
\frac{\partial}{\partial b_{i}}\mathcal{D}(\mathbf{W}, \mathbf{b})
= \sum_{k=1}^N \sum_{m=1}^{10}
(y_m^{(k)} - \bar{y}_m^{(k)})
\cdot
\phi^{\prime}(y_m^{(k)}) \cdot
\frac{\partial}{\partial b_{i}}(\sum_{n=1}^{784} w_{mn} x^{(k)}_n + b_m)
\end{equation}

which again simplifies greatly by virtue of equations \ref{eq:110} and \ref{eq:130}:

\begin{equation}\label{eq:220}
\frac{\partial}{\partial b_{i}}\mathcal{D}(\mathbf{W}, \mathbf{b})
= \sum_{k=1}^N
(y_i^{(k)} - \bar{y}_i^{(k)})
\cdot
\phi^{\prime}(y_i^{(k)})
\end{equation}

in summary yielding in concise vector notation (repeating \ref{eq:180}):

\begin{equation}
\mathbf{y} = f(\mathbf{x})=\Phi(\mathbf{W} \cdot \mathbf{x} + \mathbf{b})
\end{equation}

\begin{equation}
\nabla_{\mathbf{W}} \mathcal{D}(\mathbf{W}, \mathbf{b})=
\sum_{k=1}^N \left[ ( \mathbf{y}_k - \bar{\mathbf{y}}_k    )  \odot \mathbf{\Phi^{\prime}}(\mathbf{W} \cdot \mathbf{x_k} + \mathbf{b}) \right]
\cdot \mathbf{x}_k^T
\end{equation}

\begin{equation}
\nabla_{\mathbf{b}} \mathcal{D}(\mathbf{W}, \mathbf{b})=
\sum_{k=1}^N  ( \mathbf{y}_k - \bar{\mathbf{y}}_k    )  \odot \mathbf{\Phi^{\prime}}(\mathbf{W} \cdot \mathbf{x_k} + \mathbf{b})
\end{equation}

Since you're reading this very sentence right now, you have made it through quite some amount of number crunching. Congratulations! I think you now deserve a truly satisfactory experience and I have one for you: I'll show you the Scala code that implements our results. In the below code snippet, * is the operator notation for the Hadamard component-wise multiplication, and ** is used for matrix multiplication and outer product.

\begin{lstlisting}[label=vectors_in_scala,caption=Transfering vector notation into code]

def nabla_b (x: T, y_bar: T, W: T, b: T ) = {
	val y = Phi(W ** x + b)
	(y - y_bar) * Phi_prime(W ** x + b)
}
def nabla_W (x: T, y_bar: T, W: T, b: T ) = {
	val y = Phi(W ** x + b)
	((y - y_bar) * Phi_prime(W ** x + b)) ** x.T
}

\end{lstlisting}

Isn't that amazing? Our mathematical results are precisely reflected in the code. We managed to delegate a lot of complex index juggling to a high-performance library and what's left is highly readable, easily comprehensible, downright beautiful code. That's why I love this job of mine!
But now that we have warmed up to the basic math toolkit, I'd rather stay on the subject and address a mighty concept that'll help us go deeper. Be reminded: you should never go deeper before you know how to propagate back...;-)

\chapter{Deep Networks and Backpropagation}

\section{Calculus's Chain Rule At It's Best}

We have seen that if we have an affine function with arbitrary weights \(w_{ij} \) and biases \(b_k \) and a sufficient number of input values (images) that we know the correct output values of, we can use the function's gradient to find those parameters that best produce the known output. 

We reasonably hope that the function also produces meaningful classifications for yet unseen input values. I use the word "hope" intentionally because the resulting function may indeed fail to generalize, i. e. fail to produce meaningful output on any new input values. This effect may be due to so-called \emph{overfitting} or local minima, which both are so well described in Michael's Blog (REF!!!) that I strongly suggest you consult the text given there. Since this text is meant to be about \emph{machine learning meets software engineering}, please forgive me that I simply assume you'll find satisfactory resources elsewhere - there are so many!

So, what we came up with until now was a function looking like:

\begin{equation}
\mathbf{f}(\mathbf{x})=\Phi(\mathbf{W} \cdot \mathbf{x} + \mathbf{b})
\end{equation}

Indeed, I will show later when we finally get into the source code on github that this approach correctly classifies about 90\% of the test images. That's not bad for a start, but certainly not good enough to use it for automatic reading of hand-written checks or any other industrial application. A well known theoretical limitation of the the \emph{affine plus activation} approach is that it can only solve problems where the input values are linearly separable. It's said that amongst other things especially this limitation of a single affine function with some activation functions have resulted in the first big disapointment with respect to the potential of machine learning and AI. 

So, the next best thing appeared to be simply plugging two or more of those functions together, but that was computationally infeasible at that time. However, exactly that's what we - amongst so many others nowaddays - are going to do here, because, obviously, sufficient computing power today comes with almost every laptop.

Explicitly, we'll be looking at functions like

\begin{equation}
\mathbf{f}(\mathbf{x})=\mathbf{\Phi_2}(\mathbf{W_2} \cdot \mathbf{\Phi_1(\mathbf{W_1} \cdot \mathbf{x} + \mathbf{b_1})} + \mathbf{b_2})
\end{equation}

or even

\begin{equation} \label{eq:300}
\mathbf{f_3}(\mathbf{x})=\mathbf{\Phi_3}(\mathbf{W_3} \cdot \mathbf{\Phi_2(\mathbf{W_2} \cdot \mathbf{\Phi_1(\mathbf{W_1} \cdot \mathbf{x} + \mathbf{b_1})} + \mathbf{b_2})} + \mathbf{b_3}) 
\end{equation}

and it won't stop there. Recognize the pattern? Just nesting affine and non-linear functions into each other. Modern neural networks have hundreds of functions plugged into each other that way. Scared? I understand, but don't worry. We're not going to go through the effort of component-wise differentiation of these expressions. But on the other hand, there's no way around calculating the gradients for each of the involved weight matrices or bias terms. We need the gradient - as we've seen - to show us how to update the weight and bias parameters, such that afterwards the results match the given true output better. If only we could find a general way of calculating the gradients in some iterative or recursive manner! Well, of course, we can and that's what's called \emph{backpropagation}.

\bigskip

At last - this is the time to introduce some other lingo you must have been missing until now: Neural networks come in so-called \emph{layers}. Historically, the term \emph{layer} has been associated with its biological counterpart in our brains. However, when we talk about functions and later about software implementation, that interpretation is difficult to transfer. Thus, within this text, I'll be using the term \emph{layer} for any elementary function that is part of a chain of functions that constitute a neural network. That essentially deviates a bit, but not too far, from the typical definition used everywhere else. I truly hope that I don't cause too much confusion by that, for I really believe it eases the understanding and implementation of neural networks significantly.

\bigskip

Let's be courageous and find all the gradients for equation \ref{eq:300}. This is a problem - admittedly. But that's what we do: We solve problems. And we're not afraid, because we have the necessary methods at hand and we're able to concentrate. The methods are \emph{divide and conquer} and \emph{reusable components}. Divide and conquer is the art of splitting a seemingly intractable problem into smaller ones in such a way that the smaller problems become solvable and (and that's the art) the small solutions fit together easily to form the solution of the original problem. Reusable components, apart from saving a lot of re-work, encapsulate knowledge by hiding some details, thus introducing a level of abstraction or granularity. Depending on what exactly someone is trying to achieve at a certain point in time, she can choose to consider the component as a well-defined black box or dive deeper into its hidden details. We say: the problem space becomes \emph{navigable}.

\bigskip

Ok then, we got our tools and methods ready. Let's split equation \ref{eq:300} into three \emph{layers} and the distance function. That seems to be one natural choice.

\begin{equation}
\begin{array}{r c l }
\mathbf{f_1}(\mathbf{x})& =& \Phi_1(\mathbf{W_1} \cdot \mathbf{x} + \mathbf{b_1}) \\ 
\mathbf{f_2}(\mathbf{f_1})& =& \Phi_2(\mathbf{W_2} \cdot \mathbf{f_1} + \mathbf{b_2}) \\
\mathbf{f_3}(\mathbf{f_2})& =& \Phi_3(\mathbf{W_3} \cdot \mathbf{f_2} + \mathbf{b_3}) \\
\mathcal{D}(\mathbf{W_1}, \mathbf{W_2}, \mathbf{W_3}, \mathbf{b_1}, \mathbf{b_2}, \mathbf{b_3}) & =&  \frac{1}{2} \sum( \mathbf{f_3} - \bar{\mathbf{y}})^2
\end{array}
\end{equation}

This reads like: First, apply function \(\mathbf{f_1}\) to the input (image), then apply \(\mathbf{f_2}\) to the result, then apply \(\mathbf{f}\) to that result, then compare the final outcome to the known correct result by calculating the distance from it.

The first three functions now look all fairly similar, so we can write down some general equations. In what follows, the somewhat slopply notation \( \frac{\partial{\mathbf{f_i} }}{\partial{\mathbf{W_i} }} \) and similar expression will make the coming equations easier to read. I'm sure you would be able to figure out exactly what they mean in terms of the involved components, if you had to.

So, generally we already know something about each layer:
\begin{equation}\label{dfdw}
	\frac{\partial{\mathbf{f_i} }}{\partial{\mathbf{W_i} }} = \mathbf{\Phi_i^\prime} \odot \mathbf{f_{i-1}^T}
\end{equation}

\begin{equation}\label{dfdf}
	\frac{\partial{\mathbf{f_i }}}{\partial{\mathbf{f}_{i-1} }} = \mathbf{\Phi}^\prime_i \odot \mathbf{W}_i
\end{equation}

That means we can calculate those terms right away wherever they appear with only the knowledge of the current layer. But to calculate the gradient we have to apply the chain rule once more:

\begin{equation}
\frac{\partial \mathcal{D}}{\partial{\mathbf{W}_i }}=
\frac{\partial \mathcal{D}}{\partial \mathbf{f}_i} \cdot 
\frac{\partial{\mathbf{f}_i }}{\partial{\mathbf{W}_i }}  
\end{equation}

and it is the first term \(\frac{\partial \mathcal{D}}{\partial \mathbf{f}_i}\) that we can't calculate with mere knowledge of the current fuction. Luckily, we can get that term from the subsequent layer. And it's exactly the handback of this term from each layer to its prior layer that we call \emph{backpropagation}. The following equations outline that backpropagation schematically. You may want to read it bottom up, where the last (here: third) layer gets this unknown term as the derivative of the distance or cost function, which is \(\mathbf{y} - \mathbf{\bar{y}} \) in case of the euclidean distance that we used previously. It turns out that this is the most efficient way to compute all the gradients of all layers. You can convince yourself that the gradients with respect to the bias follow the same propagation pattern. We'll see it when we start coding, which won't take long anymore.

\bigskip

\begin{equation}\label{key}
\begin{array}{cccccccccccccc}
	 \frac{\partial \mathcal{D}}{\partial{\mathbf{W_1} }} & = & 
	 \frac{\partial \mathcal{D}}{\partial \mathbf{f_1}}  & \cdot & 
	 \frac{\partial{\mathbf{f_1} }}{\partial{\mathbf{W_1} }}  
\\ 
\\
	 & & \uparrow
\\
	& &
	\frac{\partial \mathcal{D}}{\partial \mathbf{f_1}} & = & 
	\frac{\partial \mathcal{D}}{\partial \mathbf{f_2}} & \cdot &
	\frac{\partial{\mathbf{f_2} }}{\partial{\mathbf{f_1} }} 
\\ 
\\
	\frac{\partial \mathcal{D}}{\partial{\mathbf{W_2} }}  & = & &  & 
	\frac{\partial \mathcal{D}}{\partial \mathbf{f_2}} & \cdot &
	\frac{\partial{\mathbf{f_i} }}{\partial{\mathbf{W_2} }} 
\\
\\
 & & & & \uparrow
\\
   &  & &  & 
	\frac{\partial \mathcal{D}}{\partial \mathbf{f_2}} & = &
	\frac{\partial \mathcal{D}}{\partial \mathbf{f_3}} & \cdot &
	\frac{\partial{\mathbf{f_3} }}{\partial{\mathbf{f_2} }} 
\\ 
\\
\frac{\partial \mathcal{D}}{\partial{\mathbf{W_3} }}  & = & &  & &  & 
\frac{\partial \mathcal{D}}{\partial \mathbf{f_3}} & \cdot &
\frac{\partial{\mathbf{f_3} }}{\partial{\mathbf{W_3} }} 
\\
\\
& & & & & & \uparrow
\\
& & & &\frac{\partial \mathcal{D}}{\partial \mathbf{f_3}}  & = & \mathbf{y} - \mathbf{\bar{y}} 
\end{array}
\end{equation}

Let's read this formula bottom up - from the last layer back to the first: The third layer uses the derivative of the cost function to compute its gradient. Then it computes the derivative of the cost function with respect to its own input, which is - obviously the output of the previous layer. This derivate is then passed back to the previous layer, such that the latter can do exactly the same computation, and so forth. Above formula tells the story of the weight gradients. You can easily do the same exercise for the gradients with respect to the bias. What's even more convenient is that the above formula indicates that we can consider the respective activation function and affine function both as separate layers in their own right. An \emph{activation layer} does not have parameters to update, though,  so no gradients need be calculated there. However, also the activation layers will need to pass the cost derivatives through to their respective previous layers, after having multiplied them in a Hadamard fashion with their own derivate. 

\section{Design and Implementation}

Now, within the context of this academic exercise, and equipped with an efficient algorithm for back propagation, I propose the following software design:

\begin{itemize}
	\item A neural network is a chain of functional layers
	\item A functional layer provides a method that computes the result by passing its result to the next layer and passing the returned value back to its previous layer. Thus allowing the entire chain to act as a single functional layer
	\item A functional layer provides a method that allows the previous layer to retrieve the cost derivative with respect to its own output
	\item A functional layer that features parameters shall be able to compute and provide its gradient 
	\item A functional layer shall provide a method that updates its parameters
\end{itemize}

As I mentioned in the introduction, I will use the Scala language and ecosystem to demonstrate a sound high-performance implementation of the concepts that we have just learned. Let's start with the fundamental concept of a \emph{Layer}, expressed in form of a Scala trait:

\begin{lstlisting}[label=vectors_in_scala,caption=The functional layer interface] 

trait Layer {

		/** return the function value for the given x */
		def func(x: T): T
		
		/** Forward pass through the entire network. */
		def fp(x: T): T
		
		/** Forward and backward pass in one go */
		def fbp(x: T, yb: T): BackPack
		
		/** return the subsequent layer in this network. */
		def next: Layer
		
		/** The "chaining" operator. Chains layers to form a neural network */
		def !!(rhs: Layer): Layer
		
		/** cost derivative with respect to this layer's input. */
		def dC_dy(x: T, dC_dy_from_next: T): T
		
		/** return a list of all gradients from subsequent layers with earlier layers first. */
		def grads(x: T, dC_dy_from_next: T): Option[(T, T)]
		
		/** Update all parameters */
		def update(grads: List[(T, T)]): Unit
		
}

\end{lstlisting}

The particular implementations for affine and activation layers make use of a simple but powerful optimization technique, therefore I postpone their discussion until we had the opportunity to address the subject of optimization in section \ref{accelerators}. 

However, if you can't wait, you'll find the code in the accompanying github account at \emph{https://github.com/smurve/yainn}. Actually, the real code has more documentation than I can show here, and there's also comprehensive markdown documentation on the implementation specifics. Here, \(T\) is a general \(m \times n\) tensor, i.e. a vector or a matrix. It's actually an alias for ND4J's INDArray abstraction that allows for optimum performance by adressing the particular hardware implementation behind the scenes. It will, e.g., automatically choose GPUs if the libraries required for that are found on the class path. The misterious class \emph{BackPack} holds the gradients and cost derivatives needed for backpropagation

This design allows us to create arbitrarily deep neural networks by simply stacking them on top of each other:

\begin{lstlisting}[label=vectors_in_scala,caption=Chaining layers to form a neural network] 

  /**
	* create the network from randomly initialized weights and biases
*/
def createNetwork (nx: Int, nh: Int, ny: Int, seed: Long): Layer = {
	
	val W0 = Nd4j.rand(...
	val b0 = Nd4j.rand(...
	val W1 = Nd4j.rand(...
	val b1 = Nd4j.rand(...
	
	Affine(W0, b0) !! Sigmoid() !! Affine(W1, b1) !! Sigmoid() !! Output(euc, euc_prime)
}
\end{lstlisting}

\bigskip

And with such a network, our optimization algorithm \ref{algo} may look similar to the below (In reality, it's a bit more involved, so you'll see something slightly different in the real code)

\begin{lstlisting}[label=vectors_in_scala,caption=Optimizing a neural network] 

val nn = createNetwork(.....)

while (data.hasNext && cost > threshold) {
	
			val (trainingImages, trainingLabels) = data.nextBatch()
	
			(grad, cost) = nn.fbp(trainingImages, trainingLabels)

			val delta = -grad * ETA

			nn.update(delta)
			...
}
\end{lstlisting}

which can be described as: Subtract a little portion (ETA) of the gradient from the current parameters to get to the next, better set of parameters. Repeat as long as the cost is above a certain threshold. Once this procedure is finished the resulting network should be able to correctly classify (almost) all yet unseen images of handwritten digits, like e.g. in the following code snippet that you'll also see in the github project. This snippet displays a number of images together with their true labels and the mostly matching classifications of our neural network. 

\begin{lstlisting}
for ( i <- 0 until n ) {
		val img = imgs(->, i)

		/* This is the actual prediction - no more than a function, just as I promised */
		val classification = nn.fp(img)

		println(visualize(img.reshape(28, 28)))
		val label = ...
		println(s"labeled as   : $label, classified as: $classification")
}
\end{lstlisting}

\section{Leveraging Linear Algebra Accelerators}\label{accelerators}


When using the variable \(\mathbf{x}\) in the text until now, we were always referring to a single input vector or image. So, when it comes to training a neural network with 60'000 images, which is the case for MNIST, we would pass 60'000 images to the network - one by one. This is very inefficient, as it doesn't make maximum use of the massive parallelization potential that comes with matrix multiplication. You can convince yourself about that potential easily by finding that each component of the resulting matrix product does not depend on any other component. It only depends on the same read-only data as the other components. It's therefore much more efficient to compute 1000 or even 2000 images in a single go. The underlying framework - i. e. any of the linear algebra libraries that can be used under the hood of ND4J's INDArray abstraction - will then decide by itself how much data it can push into the CPU/GPU's cache to process in a single go. 

Interestingly, it's the mere fact that we're using linear algebra here that allows us to achieve the desired optimization. Let's have a look at a single affine function in isolation:

\begin{equation}
\mathbf{f}(\mathbf{x})=\mathbf{W} \cdot \mathbf{x} + \mathbf{b}
\end{equation}

Here, \(\mathbf{x}\) is a single column vector representing a single image. Now imagine, we're taking a number \(N\)of those vectors side by side. Obviously, taken together, they form a matrix. Multiplication of two matrices is well defined and it turns out that the result of multiplying \(\mathbf{W}\) with the matrix \(\mathbf{X}\) defined as

\begin{equation}
	\mathbf{X} := (  \mathbf{x_1}, \mathbf{x_2}, \dots,  \mathbf{x_N}, )
\end{equation}

is just the matrix 

\begin{equation}
\mathbf{Y} := \mathbf{W} \cdot \mathbf{X} = (  \mathbf{y_1}, \mathbf{y_2}, \dots,  \mathbf{y_N}, )
\end{equation}

consisting of all of the results \(\mathbf{y_i}\) that we would have computed by multiplying by \(\mathbf{W}\) one by one. So, that was easy. Only, adding a bias \(\mathbf{b}\) wouldn't apply to each of the resulting \(\mathbf{y_i}\). Indeed, adding a vector to a matrix is not even well defined, so that's not an option. It takes the help of another little trick, to add the bias to every single of the \(\mathbf{y_i}\)

If we stack the bias to the left of the weight matrix and pad the \(\mathbf{X}\) with a top row of \(1\)s, then we'll arrive at the following vector notation:

\begin{equation} \label{parallel_affine}
\mathbf{f}(\mathbf{x})=(\mathbf{b};\mathbf{W}) \cdot \left( 
\begin{array}{c} 
1, 1, \dots, 1 \\
\mathbf{X} 
\end{array}
 \right)
\end{equation}

or in component notation 

\begin{equation}
\mathbf{f}(\mathbf{x})=
\left( 
\begin{array}{ccccc} 
b_1 & w_{11} & w_{12} & \dots &  w_{1n}
\\
b_2 & w_{21} & w_{22} & \dots &  w_{2n}
\\
\dots & \dots &\dots &\dots &\dots
\\
b_m & w_{m1} & w_{m2} & \dots &  w_{mn}
\end{array}
\right)
\cdot
\left( 
\begin{array}{cccc} 
1 & 1 & \dots & 1
\\
x_1^{(1)}& x_1^{(2)}& \dots & x_1^{(N)}
\\
x_2^{(1)}& x_2^{(2)}& \dots & x_2^{(N)}
\\
\dots &\dots &\dots &\dots
\\
x_n^{(1)}& x_n^{(2)}& \dots & x_n^{(N)}
\end{array}
\right)
\end{equation}

and with summation being performed over the range \(i \in [1, n]\):

\begin{equation}
= \left(
\begin{array}{cccc} 
b_1 + \sum w_{1i} \cdot x_i^{(1)} & b_1 + \sum w_{1i} \cdot x_i^{(2)} & \dots & b_1 + \sum w_{1i} \cdot x_i^{(N)}
\\
b_2 + \sum w_{2i} \cdot x_i^{(1)} & b_2 + \sum w_{2i} \cdot x_i^{(2)} & \dots & b_2 + \sum w_{2i} \cdot x_i^{(N)}
\\
\dots & \dots & \dots & \dots
\\
b_m + \sum w_{mi} \cdot x_i^{(1)} & b_m + \sum w_{mi} \cdot x_i^{(2)} & \dots & b_m + \sum w_{mi} \cdot x_i^{(N)}
 \end{array}
\right)
\end{equation}

\begin{equation}
= \left(
\mathbf{f}(\mathbf{x^{(1)}}), \mathbf{f}(\mathbf{x^{(2)}}), \dots, \mathbf{f}(\mathbf{x^{(N)}})
\right)
\end{equation}

which is a matrix composed of all the column vectors that we'd get from applying the affine function to each vector \(\mathbf{x}\) one by one. And that is exactly what we set out to achieve!

In summary: Using equation \ref{parallel_affine}, we have a notation that allows us to to compute the affine function for any number of input vectors in one go. And the good news is that, again, we can simply use the same formula to implement this idea in our code.

\begin{lstlisting}[label=fn_optimized, caption=Optimizing affine functions]
def func(x: T): T = {
	h(b, W) ** v1(x)
}
\end{lstlisting}

Here, the \(\mathbf{h}\) is a helper function that stacks the bias \(\mathbf{b} \) left to the weight matrix \(\mathbf{W} \) and  \(\mathbf{v1} \) is a function that stacks \(1\)s on top of \(\mathbf{x} \), the latter being the matrix consisting of an entire mini-batch of images.

\bigskip

And this, finally, concludes the theory. We have learned by now how to find a good set of parameters for nested affine functions with non-linear activation functions inbetween, such that applying the resulting deep network function to any number of yet-unseen images returns a matrix of one-hot vectors that represent the known classifications - as good as it can get with the given architecture.  

Please keep in mind that our discussions didn't cover performance, maintainability or even mathematical soundness to a perfectly satisfactory extend. I'm not saying that Scala is the only language that shall prevail. And I'm not saying that the code examples are perfect to go into production right away. After all, this blog and its accompanying code constitute no more than an academical exercise. An exercise, though, that I found worth writing and I hope you - in retrospect - find worth reading.


\section{Where to Go From Here}

I invite you now to go to github and check out my little Scala project, where a markdown document will pick you up and guide you through the implementation of layers and through the experiments that I demonstrate with their help. You'll see much more more than what you typically expect in an introductory Hello World example. You'll see some ... \emph{pause} ... \emph{drums} ... unit tests! You see gradient checks and a lot of scaladoc comments everywhere. With the additional complexity, I want to remind you of what this post was actually about:


\bigskip

\emph{When software engineering meets machine learning, there's practices to merge and a whole bunch of amazing new things to learn for everybody! }



\begin{thebibliography}{9}
	
	
	\bibitem{nielson2015}
	Michael A Nielson, \textit{Neural Networks and Deep Learning}.
	Determination Press, 2015
	\\\texttt{http://neuralnetworksanddeeplearning.com/}
	
	\bibitem{olah2015}
	Christopher Olah. \textit{colah's Blog}
	Personal blog
	
	\bibitem{goodfellow}
	Ian Goodfellow, Yoshua Bengio, Aaron Courville, \textit{	Deep Learning }
	Online book MIT Press 2016,
	\texttt{http://www.deeplearningbook.org}

	\bibitem{gartner2017}
	Gartner's Hype Cycle. Web Resource.
	\\\texttt{ (http://www.gartner.com/smarterwithgartner/top-trends-in-the-gartner-hype-cycle-for-emerging-technologies-2017/)}
	
	\bibitem{wampler2016}
	Dean Wampler. \textit{Scala, the unpredicted lingua franca for data science}
	 \\\texttt{https://www.youtube.com/watch?v=3\_oV25nZz8I\&t=741s}
	Youtube recording. Scala Days NY 2016
	
	\bibitem{nd4j}
	ND4J Development Team. ND4J: N-dimensional arrays and scientific computing for the JVM, Apache Software Foundation License 2.0.* 
	\\\texttt{http://nd4j.org}
	
	\bibitem{yainn2017}
	Wolfgang I. Giersche, \textit{yainn}. Github Repo
	\\\texttt{https://github.com/smurve/yainn}
	
	\bibitem{mnist89}
	LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998). \textit{Gradient-based learning applied to document recognition}. Proceedings of the IEEE, 86, 22782324.	
	\\Data files and results at \texttt{http://yann.lecun.com/exdb/mnist/}
\end{thebibliography}

\end{document}





